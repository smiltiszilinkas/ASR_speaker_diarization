{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "from pydub import AudioSegment\n",
    "import matplotlib.pyplot as plt\n",
    "# from audioProcessing import trim_and_save_wav\n",
    "from timeConversion import time_to_miliseconds, seconds_to_miliseconds\n",
    "from videoAudioData import video_data\n",
    "from audioProcessing import read_audio_meta_from_json, reorder_labels, calculate_accuracy, get_intervals_from_audio,combine_random_audio_clips, add_noise_to_combined_files\n",
    "from constants import API_KEY_SPEAKER_DIARIZATION_READ\n",
    "import json\n",
    "\n",
    "access_token = API_KEY_SPEAKER_DIARIZATION_READ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA\n",
    "The data source files are excluded but store it in data/videoAudio/[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. video_audio_0.wav - https://www.youtube.com/watch?v=Xnk4seEHmgw - Jim's Pranks Against Dwight - The Office US\n",
    "1. video_audio_1.wav - https://www.youtube.com/watch?v=-uleG_Vecis - 100 Computer Science Concepts Explained\n",
    "2. video_audio_2.wav - https://www.youtube.com/watch?v=hxi4RzZaO04 - How She Made $88 Million From A Pilates Company\n",
    "3. video_audio_3.wav - https://www.youtube.com/watch?v=zaB_20bkoA4 - Elon Musk's BRUTALLY Honest Interview With Tucker Carlson (2023)\n",
    "4. video_audio_4.wav - https://www.youtube.com/watch?v=o3K_HbpWNpg - Think Fast. Talk Smart | Matt Abrahams | TEDxMontaVistaHighSchool\n",
    "5. video_audio_5.wav - https://www.youtube.com/watch?v=ssgRCoRh_EY - Dua Lipa on Her Very Detailed Daily Schedule, Notebook of Song Lyrics, New Album & Albanian Sayings\n",
    "6. video_audio_6.wav - https://www.youtube.com/watch?v=fZLLqapzxRQ - Little Big Shots Meet Micro Mayor James Episode Highlight( Engsub)\n",
    "7. video_audio_7.wav - https://www.youtube.com/watch?v=WttV8_J9674 - Kardashian-Jenner Sisters' BIGGEST Fights | KUWTK | E!\n",
    "8. video_audio_8.wav - https://www.youtube.com/watch?v=uOOCL7PLNNo - Will Gordon Ramsay Find the Lamb Sauce Cooking Lamb Chops?\n",
    "9. video_audio_9.wav - https://www.youtube.com/watch?v=noR_3lkeAoY - Show-Stopping Lamb Dish Stuns Marcus Wareing | MasterChef UK\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into the audio tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type:\n",
    "woman - denotes woman speech;\n",
    "man - denotes man speech;\n",
    "kid - denotes kid speech;\n",
    "\n",
    "Tembre:\n",
    "m - denotes male speech;\n",
    "f - denotes female speech;\n",
    "\n",
    "Duration:\n",
    "s - short for not longer than 5s\n",
    "l - for longer than 5s\n",
    "\n",
    "example: woman1_f_s_1.wav - denotes woman 1 is speaking, female, for shorter than 6s, 1st audio track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "for audio_clip in video_data:\n",
    "  t1 = time_to_miliseconds(audio_clip[5]) #Works in milliseconds\n",
    "  t2 = time_to_miliseconds(audio_clip[6])\n",
    " \n",
    "  new_audio = AudioSegment.from_file(f'data/videoAudio/{audio_clip[0]}')\n",
    "  new_audio = new_audio[t1:t2]\n",
    "  # Convert audio to raw PCM data\n",
    "  new_audio_raw = new_audio.raw_data\n",
    "\n",
    "  # Convert raw PCM data to numpy array\n",
    "  audio_array = np.frombuffer(new_audio_raw, dtype=np.int16)\n",
    "\n",
    "  # Clip new_audio audio data to ensure it stays within the appropriate range [-32768, 32767]\n",
    "  audio_array = np.clip(audio_array, -32768, 32767)\n",
    "\n",
    "  # Convert the audio_array audio array back to bytes\n",
    "  new_audio_raw = audio_array.astype(np.int16).tobytes()\n",
    "\n",
    "  # Create a new AudioSegment from the noisy audio raw data\n",
    "  if new_audio.channels == 1:\n",
    "      # If the original audio is mono, set the new audio to mono\n",
    "      n_audio = AudioSegment(new_audio_raw, frame_rate=new_audio.frame_rate, sample_width=new_audio.sample_width, channels=1)\n",
    "  else:\n",
    "      # If the original audio is stereo, set the new audio to stereo\n",
    "      n_audio = AudioSegment(new_audio_raw, frame_rate=new_audio.frame_rate, sample_width=new_audio.sample_width, channels=2)\n",
    "  \n",
    "  # Save the new audio to a new .wav file\n",
    "  n_audio.export(f'data/audioClips/{audio_clip[4]}.wav', format=\"wav\") #Exports to a wav file in the current path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clips\n",
    "number_clips = 500\n",
    "combine_random_audio_clips(number_clips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did extract the data from the videos. Concatenated randomly 30 combined audios. Normalized concatenated audios. Analyzed using tools.\n",
    "Added noise. Analyzed the results. Gender and so on.\n",
    "\n",
    "After inspecting the predicted labels I have observed speaker labels starting not from zero, sometimes the algorithm was predicting labels mixed or top to bottom, therefore I have wrote an algorithm to rename speakers based on the timing they were predicted. Which bumped the score significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization at 0x227e037b200>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the CUDA visible devices\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    if num_gpus <= 1:\n",
    "        print(f'Using {num_gpus} GPU')\n",
    "    else:\n",
    "        print(f'Using {num_gpus} GPUs')\n",
    "\n",
    "# You can also explicitly set a device\n",
    "# For example, to use GPU 0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=access_token)\n",
    "\n",
    "pipeline.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict without noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found only 5 clusters. Using a smaller value than 11 for `min_cluster_size` might help.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/combinedAudioClips/audio_meta_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clip \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_clips):\n\u001b[0;32m      3\u001b[0m   file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/combinedAudioClips/audio_meta_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclip\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m   audio_meta \u001b[38;5;241m=\u001b[39m \u001b[43mread_audio_meta_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m   true_labels \u001b[38;5;241m=\u001b[39m get_intervals_from_audio(audio_meta)\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;66;03m# apply pretrained pipeline\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ASR_speaker_diarization\\audioProcessing.py:55\u001b[0m, in \u001b[0;36mread_audio_meta_from_json\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_audio_meta_from_json\u001b[39m(file_path):\n\u001b[1;32m---> 55\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     56\u001b[0m       audio_meta \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m audio_meta\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/combinedAudioClips/audio_meta_1.json'"
     ]
    }
   ],
   "source": [
    "# for each created combined audio\n",
    "for clip in range(number_clips):\n",
    "  file_path = f'data/combinedAudioClips/audio_meta_{clip}.json'\n",
    "  audio_meta = read_audio_meta_from_json(file_path)\n",
    "  true_labels = get_intervals_from_audio(audio_meta)\n",
    "\n",
    "  # apply pretrained pipeline\n",
    "  diarization = pipeline(f'data/combinedAudioClips/combined_audio_{clip}.wav', num_speakers=len(true_labels))\n",
    "  predicted_labels = []\n",
    "  # predict labels\n",
    "  for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "      prediction = f\"start={seconds_to_miliseconds(turn.start)}ms stop={seconds_to_miliseconds(turn.end)}ms {speaker}\"\n",
    "      predicted_labels.append(prediction)\n",
    "\n",
    "  # correct the order\n",
    "  predicted_labels = reorder_labels(predicted_labels)\n",
    "\n",
    "  accuracy = calculate_accuracy(true_labels, predicted_labels)\n",
    "\n",
    "  predictions = {\n",
    "     'true_labels': true_labels,\n",
    "     'predicted_labels': predicted_labels,\n",
    "     'accuracy': float(f'{accuracy:.3f}'),\n",
    "     'number_of_speakers': len(true_labels)\n",
    "  }\n",
    "  # write results to json\n",
    "  with open(f\"data/predictions/preds_{clip}.json\", \"w\") as file:\n",
    "      json.dump(predictions, file)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_levels = [0.01,0.05,0.1,0.3,0.5,0.7,0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found only 6 clusters. Using a smaller value than 12 for `min_cluster_size` might help.\n",
      "Found only 3 clusters. Using a smaller value than 12 for `min_cluster_size` might help.\n",
      "Found only 6 clusters. Using a smaller value than 12 for `min_cluster_size` might help.\n"
     ]
    }
   ],
   "source": [
    "for noise in noise_levels:\n",
    "  add_noise_to_combined_files(noise,number_clips)  # Add % of noise to the combined audio\n",
    "  # for each created combined audio\n",
    "  for clip in range(number_clips):\n",
    "    file_path = f'data/combinedAudioClips/audio_meta_{clip}.json'\n",
    "    audio_meta = read_audio_meta_from_json(file_path)\n",
    "    true_labels = get_intervals_from_audio(audio_meta)\n",
    "\n",
    "    # apply pretrained pipeline\n",
    "    diarization = pipeline(f'data/combinedAudioClips/combined_audio_noise_{str(noise)}_{clip}.wav', num_speakers=len(true_labels))\n",
    "    predicted_labels = []\n",
    "    # predict labels\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        prediction = f\"start={seconds_to_miliseconds(turn.start)}ms stop={seconds_to_miliseconds(turn.end)}ms {speaker}\"\n",
    "        predicted_labels.append(prediction)\n",
    "\n",
    "    # correct the order\n",
    "    predicted_labels = reorder_labels(predicted_labels)\n",
    "\n",
    "    accuracy = calculate_accuracy(true_labels, predicted_labels)\n",
    "\n",
    "    predictions = {\n",
    "      'true_labels': true_labels,\n",
    "      'predicted_labels': predicted_labels,\n",
    "      'accuracy': float(f'{accuracy:.3f}'),\n",
    "      'number_of_speakers': len(true_labels)\n",
    "    }\n",
    "    # write results to json\n",
    "    with open(f\"data/predictions/preds_noise_{str(noise)}_{clip}.json\", \"w\") as file:\n",
    "        json.dump(predictions, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
